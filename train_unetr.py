from argparse import Namespace
from dataclasses import asdict
import json
import sys
from typing import Any
from exact_datasets.nct2013 import cohort_selection
from omegaconf import OmegaConf
from simple_parsing import ArgumentParser, config_for, Partial, field, subgroups
import numpy as np
from rich_argparse import ArgumentDefaultsRichHelpFormatter
from timm import data
import torch
from torch.optim import lr_scheduler
from src.models import unetr
from src.models.medsam import MedSAMIBot
from src.models.vision_transformer import VisionTransformer
from src.datasets import (
    NCT2013ImagesAndCancerMasksDataset,
    MicroSegNetDataset,
)
from src.transform import (
    get_pixel_level_augmentations_for_greyscale_images,
)
import pandas as pd
from src.optim import cosine_scheduler, dataclass
import wandb
from src.ssl_evaluation import compute_binary_classification_metrics
from torch.nn import functional as F
from tqdm import tqdm
import logging
import matplotlib.pyplot as plt
from exact_datasets.nct2013.cohort_selection_v2 import (
    CohortSelector,
    CohortSelectionOptions,
)
from src.models import get_model, _MODEL_REGISTRY
from src.argparse_utils import add_args, get_kwargs
from torch import distributed as dist
import os
from src.utils import has_batchnorms
from torchvision.transforms import v2 as T
from PIL import Image
from torchvision import tv_tensors as tvt
from monai.metrics import DiceMetric


GT_LABEL_IGNORE_IDX = 255


def get_arg_parser():
    parser = ArgumentParser(
        formatter_class=ArgumentDefaultsRichHelpFormatter, add_help=False
    )

    parser.add_argument(
        "--mode",
        type=str,
        choices=["px_binary_clf", "segmentation"],
        default="px_binary_clf",
        help="One of 'px_binary_clf' or 'segmentation'",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=["nct2013", "microsegnet"],
        default="nct2013",
        help="Dataset to use for training",
    )
    parser.add_argument("--data_path", type=str, help="Path to the dataset")
    parser.add_argument(
        "--use_amp", action="store_true", help="Use automatic mixed precision"
    )
    parser.add_argument(
        "--use_distributed",
        action="store_true",
        help="Use distributed training (env variables must be set)",
    )
    parser.add_argument(
        "--augmentations", choices=("none", "translate", "random_crop"), default="none", 
    )
    parser.add_argument(
        "--mean", type=float, nargs='+', default=[0, 0, 0], help="Mean for normalization"
    )
    parser.add_argument(
        "--std", type=float, nargs='+', default=[1, 1, 1], help="Std for normalization"
    )

    # ======= Model options =======
    group = parser.add_argument_group("Model")
    group.add_argument(
        "--model_path",
        type=str,
        default=None,
        help="Path to the saved model (pickled model, not state dict)",
    )
    group.add_argument(
        "--arch",
        type=str,
        choices=_MODEL_REGISTRY.keys(),
        default=None,
        help="Architecture of the model - if None, it will be inferred from the model_path",
    )
    group.add_argument(
        "--model_weights_path",
        type=str,
        default=None,
        help="Path to the weights of the model - if None, it will be inferred from the model_path",
    )
    group.add_argument(
        "--norm_name",
        type=str,
        choices=("batch", "instance", "group"),
        default="instance",
    )

    def _dict_from_str(s):
        if not s:
            return {}
        dotlist = s.split(",")
        dotlist = [x.replace(":", "=") for x in dotlist]
        kw = OmegaConf.from_dotlist(dotlist)
        return OmegaConf.to_container(kw, resolve=True)

    group.add_argument(
        "--model_kwargs",
        type=_dict_from_str,
        default="",
        help="Additional arguments to pass to the model constructor (e.g. 'hidden_size:128,num_layers:2,dropout:0.1')",
    )
    group.add_argument(
        "--img_size", type=int, default=224, help="Size of the input images"
    )
    group.add_argument(
        "--feature_size",
        type=int,
        default=14,
        help="Size of the feature maps generated by the vision transformer.",
    )
    group.add_argument(
        "--output_size",
        type=int,
        default=224,
        help="Size of the output feature maps (and the final output image)",
    )

    group = parser.add_argument_group("Training")
    group.add_argument(
        "--epochs", type=int, default=20, help="Number of epochs to train for"
    )
    group.add_argument("--lr", type=float, default=1e-4, help="Learning rate")
    group.add_argument(
        "--num_workers",
        type=int,
        default=8,
        help="Number of workers for the data loader",
    )
    group.add_argument(
        "--backbone_lr", type=float, default=1e-5, help="Learning rate for the backbone"
    )
    group.add_argument("--wd", type=float, default=1e-4, help="Weight decay")
    group.add_argument(
        "--loss_weights",
        type=float,
        nargs="+",
        default=None,
        help="Weight for the positive class in the loss function",
    )
    group.add_argument(
        "--batch_size", type=int, default=4, help="Batch size for training"
    )
    group.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="Device to use for training",
    )

    # check the dataset and maybe add more arguments
    args, _ = parser.parse_known_args()
    if args.dataset == "nct2013":
        group = parser.add_argument_group("NCT2013 data options")
        group.add_argument(
            "--splits_file",
            type=str,
            default=None,
            help="Path to the splits file - if set, the splits will be loaded from this file (ignoring the cohort selection options)",
        )
        group.add_argument(
            "--set_for_validation",
            default="val",
            help="Which set to use for validation",
        )

    group = parser.add_argument_group("Logging")
    group.add_argument(
        "--log_images_every",
        type=int,
        default=0,
        help="Log images to wandb every n iterations. Set to 0 to disable.",
    )

    parser.add_argument(
        "--help", "-h", action="help", help="Show this help message and exit"
    )

    return parser


def train(args):
    if args.use_distributed:
        dist.init_process_group(backend="nccl")
        torch.cuda.set_device(dist.get_rank())
        if dist.get_rank() != 0:
            os.environ["WANDB_MODE"] = "disabled"
        else:
            logging.basicConfig(stream=sys.stdout, level=logging.INFO)

    wandb.init(project="ibot", config=args, job_type="train_unetr")

    unetr_model = get_unetr_model(args).to(args.device)
    if args.use_distributed:
        unetr_model = torch.nn.parallel.DistributedDataParallel(
            unetr_model, device_ids=[dist.get_rank()], output_device=dist.get_rank()
        )
        if has_batchnorms(unetr_model):
            logging.info("Converting model to SyncBatchNorm")
            unetr_model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(unetr_model)

    train_loader, val_loader = make_data_loaders(args)
    niter_per_epoch = len(train_loader)

    optimizer, lr_scheduler = get_optimizer(unetr_model.module if args.use_distributed else unetr_model, niter_per_epoch, args)
    if args.use_amp:
        grad_scaler = torch.cuda.amp.GradScaler()
    else:
        grad_scaler = None

    criterion = torch.nn.CrossEntropyLoss(
        weight=(
            torch.tensor(args.loss_weights).to(args.device)
            if args.loss_weights is not None
            else None
        ),
        ignore_index=GT_LABEL_IGNORE_IDX,
    )

    for epoch in range(args.epochs):
        train_metrics = train_or_eval_loop(
            unetr_model,
            train_loader,
            criterion,
            optimizer,
            lr_scheduler,
            desc="train",
            device=args.device,
            log_images_every=args.log_images_every,
            mode=args.mode,
            grad_scaler=grad_scaler,
        )
        val_metrics = train_or_eval_loop(
            unetr_model,
            val_loader,
            criterion,
            desc="val",
            device=args.device,
            log_images_every=args.log_images_every,
            mode=args.mode,
            grad_scaler=None,
        )
        train_metrics['epoch'] = epoch
        val_metrics['epoch'] = epoch
        wandb.log({f"train/{k}": v for k, v in train_metrics.items()})
        wandb.log({f"val/{k}": v for k, v in val_metrics.items()})


def train_or_eval_loop(
    model,
    loader,
    criterion,
    optimizer=None,
    lr_scheduler=None,
    desc="train",
    device="cuda",
    log_images_every=0,
    mode="segmentation",
    grad_scaler=None,
):
    training = optimizer is not None
    model.train(training)

    if mode == "segmentation":
        tracker = SegmentationMetricsTracker(ignore_index=GT_LABEL_IGNORE_IDX)
    else:
        tracker = BinaryMetricsTracker(ignore_index=GT_LABEL_IGNORE_IDX)

    with torch.set_grad_enabled(training):
        for i, (im, mask) in enumerate(tqdm(loader, desc=desc)):
            im, mask = im.to(device), mask.to(device)

            with torch.cuda.amp.autocast(enabled=grad_scaler is not None):
                pred = model(im)
                loss = criterion(pred, mask)

            step_metrics = {"loss": loss.item()}
            if training:
                step_metrics["lr"] = optimizer.param_groups[0]["lr"]
                optimizer.zero_grad()
                if grad_scaler is not None:
                    grad_scaler.scale(loss).backward()
                    grad_scaler.step(optimizer)
                    grad_scaler.update()
                else:
                    loss.backward()
                    optimizer.step()
                lr_scheduler.step()
            tracker.update(pred.softmax(1), mask)
            wandb.log(step_metrics)

            if log_images_every > 0 and i % log_images_every == 0:
                fig = show_images(im, mask, pred.detach())
                wandb.log({f'{"train" if training else "val"}_images': wandb.Image(fig)})
                plt.close()

        metrics = tracker.compute()
        return metrics


def get_unetr_model(args):
    if args.model_path is not None:
        model = torch.load(args.model_path)
    else:
        model = get_model(args.arch, **args.model_kwargs)
        if args.model_weights_path is not None:
            model.load_state_dict(torch.load(args.model_weights_path))

    if isinstance(model, VisionTransformer):
        image_encoder = unetr.VITImageEncoderWrapperForUNETR(vit=model)
        embed_dim = model.embed_dim
    elif isinstance(model, MedSAMIBot):
        image_encoder = model.image_encoder_wrapped.image_encoder
        embed_dim = model.embed_dim
        image_encoder = unetr.SAMWrapperForUNETR(image_encoder)
    else:
        raise NotImplementedError(
            "Only VisionTransformer backbones are supported so far."
        )
    unetr_model = unetr.UNETR(
        image_encoder,
        embedding_size=embed_dim,
        feature_size=args.feature_size,
        input_size=args.img_size,
        output_size=args.output_size,
        out_channels=2,
        norm_name=args.norm_name
    )

    logging.info(
        f"UNETR model created with {sum(p.numel() for p in unetr_model.parameters())} parameters."
    )
    return unetr_model


def make_data_loaders(args):
    data_path = args.data_path

    if args.dataset == "nct2013":
        if args.augmentations == "none":
            _transform_train = ImageMaskTransform(size=args.img_size)
        elif args.augmentations == "translate":
            _transform_train = ImageMaskTransform(
                size=args.img_size, translate=True
            )
        elif args.augmentations == "random_crop":
            _transform_train = ImageMaskTransform(
                size=args.img_size, random_crop_scale=(0.8, 1.0)
            )
        else: 
            raise ValueError(f"Unknown augmentation type: {args.augmentations}")
        _transform_val = ImageMaskTransform(size=args.img_size, mean=args.mean, std=args.std)

        class TransformWrapper:
            def __init__(self, transform):
                self.transform = transform

            def __call__(self, im, mask, metadata):
                im, mask = self.transform(im, mask)
                if mask.shape[-1] != args.output_size:
                    mask = F.interpolate(
                        mask[None, None].float(),
                        size=(args.output_size, args.output_size),
                        mode="nearest",
                    )[0, 0].long()
                return im, mask

        transform_train = TransformWrapper(_transform_train)
        transform_val = TransformWrapper(_transform_val)

        if args.splits_file is None:
            cohort_selector = CohortSelector(
                pd.read_csv("/ssd005/projects/exactvu_pca/nct2013/metadata.csv")
            )
            cohort_selection_options = CohortSelectionOptions(
                **get_kwargs(args, CohortSelectionOptions)
            )
            train_cores, val_cores, test_cores, _ = cohort_selector.select_cohort(
                cohort_selection_options
            )
        else:
            with open(args.splits_file, "r") as f:
                splits = json.load(f)
                train_cores = splits["train"]
                val_cores = splits["val"]
                test_cores = splits["test"]

        train_dataset = NCT2013ImagesAndCancerMasksDataset(
            data_path, core_ids=train_cores, transform=transform_train
        )
        val_dataset = NCT2013ImagesAndCancerMasksDataset(
            data_path,
            core_ids=val_cores if args.set_for_validation == "val" else test_cores,
            transform=transform_val,
        )

    elif args.dataset == "microsegnet":
        train_dataset = MicroSegNetDataset(
            data_path, split="train", transform=ImageMaskTransform(size=args.img_size)
        )
        val_dataset = MicroSegNetDataset(
            data_path, split="test", transform=ImageMaskTransform(size=args.img_size)
        )

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True,
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=True,
    )
    logging.info(
        f"Train loader has {len(train_loader)} batches, val loader has {len(val_loader)} batches."
    )
    return train_loader, val_loader


def get_optimizer(
    model: unetr.UNETR,
    niter_per_epoch,
    args,
):
    backbone_params = model.image_encoder.parameters()
    remaining_params = [p for p in model.parameters() if p not in backbone_params]
    params = [
        {"params": remaining_params, "lr": args.lr},
        {"params": backbone_params, "lr": args.backbone_lr},
    ]
    optimizer = torch.optim.AdamW(params, lr=args.lr, weight_decay=args.wd)

    cosine_schedule = cosine_scheduler(1, 0, args.epochs, niter_per_epoch, 5)
    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(
        optimizer, lambda it: cosine_schedule[it] if it < len(cosine_schedule) else 0
    )

    logging.info(f"Optimizer and LR scheduler created.")
    return optimizer, lr_scheduler


class BinaryMetricsTracker:
    def __init__(self, ignore_index=255):
        self.ignore_index = ignore_index
        self._mean_predictions_in_non_ignore = []
        self._mean_targets_in_non_ignore = []

    def update(self, pred, target):
        """Update the metrics with the predictions and targets"

        Args:
            pred: torch.Tensor, shape (B, C, H, W). These are post-softmax probabilities.
            target: torch.Tensor, shape (B, H, W). The target labels. Each value is either 0, 1 or ignore_index.
        """

        for p, t in zip(pred, target):
            p = p.view(2, -1).permute(1, 0)  # unroll the predictions across h, w
            t = t.view(-1)  # unroll the targets across h, w
            mask = t != self.ignore_index
            p = p[mask][:, 1]  # only keep the probability of the positive class
            t = t[mask]
            self._mean_predictions_in_non_ignore.append(p.mean().item())
            self._mean_targets_in_non_ignore.append((t.float().mean() > 0.5).item())

    def compute(self):
        # mean predictions and targets in non-ignore regions, by items
        metrics = compute_binary_classification_metrics(
            self._mean_predictions_in_non_ignore, self._mean_targets_in_non_ignore
        )
        return metrics


class SegmentationMetricsTracker:
    def __init__(self, ignore_index=255, num_classes=2):
        self.ignore_index = ignore_index
        self.dice_metric = DiceMetric(
            num_classes=num_classes
        )
    def update(self, pred, target):
        # pred are probabilities, convert them to predictions 
        pred = pred.argmax(1)
        self.dice_metric.update(pred, target)

    def compute(self):
        return {"dice_score": self.dice_metric.compute()}


class DiceMetric: 
    def __init__(self, num_classes): 
        self.num_classes = num_classes
        self._dice_scores = []

    def update(self, y_pred, y_true): 
        """
        Adds the predictions and targets to the metric.

        Args:
            y_pred: torch.Tensor, shape (B, H, W). The predicted labels.
            y_true: torch.Tensor, shape (B, H, W). The target labels.
        """
        B = y_pred.shape[0]
        dice_score_for_batch = torch.zeros(B, self.num_classes)
        for class_index in range(self.num_classes): 
            y_true_for_class = (y_true == class_index).float()
            y_pred_for_class = (y_pred == class_index).float()
            dice_score_for_class = 2 * (y_true_for_class * y_pred_for_class).sum() / (y_true_for_class.sum() + y_pred_for_class.sum())
            dice_score_for_batch[:, class_index] = dice_score_for_class

        self._dice_scores.append(dice_score_for_batch)

    def compute(self): 
        dice_scores = torch.cat(self._dice_scores, dim=0)
        # aggregate across batches and classes
        return dice_scores.mean()


def show_images(im, mask, pred):
    """Show the image, mask and prediction in a grid.

    Args:
        im: torch.Tensor, shape (B, C, H, W)
        mask: torch.Tensor, shape (B, H, W)
        pred: torch.Tensor, shape (B, C, H, W)
        mode: str, one of "segmentation" or "px_binary_clf"
    """

    fig, axes = plt.subplots(3, 1, figsize=(10, 10))

    # normalize the image to [0, 1], and permute to (H, W, C), then convert to numpy
    im = (im - im.min()) / (im.max() - im.min())
    im = im.permute(0, 2, 3, 1).cpu().numpy()[0]
    mask = mask.cpu().numpy()[0]
    pred = pred.softmax(1).cpu().numpy()[0, 1]  # probability of the positive class

    imshow_kw = dict(vmin=0, vmax=1)
    axes[0].imshow(im, **imshow_kw)
    axes[0].set_title("Image")

    # make color map for the mask
    # what is cmap for object category segmentation?
    # https://matplotlib.org/stable/tutorials/colors/colormaps.html
    axes[1].set_title("Mask")
    mask[mask == GT_LABEL_IGNORE_IDX] = -1
    plt.colorbar(axes[1].imshow(mask, cmap="tab20", vmin=-1, vmax=2), ax=axes[1])

    axes[2].imshow(pred, **imshow_kw)
    axes[2].set_title("Prediction")

    for ax in axes:
        ax.axis("off")

    fig.tight_layout()
    return fig


class ImageMaskTransform:
    """Default transform for image-mask pairs, PIL to tensor"""

    def __init__(
        self,
        size: int = 512,
        mean: tuple[float, float, float] = (0, 0, 0),
        std: tuple[float, float, float] = (1, 1, 1),
        random_crop_scale: tuple[float, float] | None = None,
        to_tensor: bool = True,
        translate: bool = False,
    ):
        self.size = size
        self.mean = mean
        self.std = std
        self.to_tensor = to_tensor
        self.translate = translate
        self.random_crop_scale = random_crop_scale

    def __call__(self, image: Image.Image, mask: Image.Image):
        image = image.convert("RGB")
        image = tvt.Image(image) / 255.0
        mask = tvt.Mask(mask)

        if self.translate:
            image, mask = T.RandomAffine(degrees=0, translate=(0.1, 0.1))(image, mask)

        if self.random_crop_scale is not None:
            image, mask = T.RandomResizedCrop(self.size, self.random_crop_scale)(image, mask)
        else:
            image, mask = T.Resize((self.size, self.size))(image, mask)

        if not self.to_tensor:
            image, mask = T.ToPILImage()(image, mask)
            return image, mask

        image, mask = T.ToTensor()(image, mask)
        image = T.ToDtype(torch.float32)(image)
        image = T.Normalize(mean=self.mean, std=self.std)(image)
        return image, mask[0].long()


def main():
    parser = get_arg_parser()
    args = parser.parse_args()
    train(args)


if __name__ == "__main__":
    main()
